# @package _global_

datamodule:
  _target_: datasets.DWIEDataModule
  _partial_: true
  dataset_dir: ./data/DWIE
  train_file: train.json
  dev_file: dev.json
  test_file: test.json
  dev_test_file: dev_test.json
  max_seq_length: 4096
  force_regeneration: false
  use_coref: true              # 
  train_batch_size: 2
  test_batch_size: 2

model:
  _target_: model.DocREModel
  _partial_: true
  model_name_or_path: ../pretrain/longformer-large-4096
  max_seq_length: 4096
  transformer_type: longformer
  graph_conv:
    # _target_: model.NoGraphConv      # 
    _target_: model.GATGraphConv
    _partial_: true
      # hidden_dim: leave for runtime decision
    edge_types: ['d-s', 's-s', 's-m', 'ie/m-m', 'is/m-m']
    feat_drop: 0.25
    attn_drop: 0.1
    residual: false
    activation:
      _target_: torch.nn.Tanh
    num_layers: 2
  residual: true
  coref: gated  # gated or e_context
  num_class: 66
  block_size: 64
  kg_conv:
    # _target_: model.NoKGEmbeddingLayer  # 
    _target_: model.KGEmbeddingLayer
    _partial_: true
    # hidden_dim: leave for runtime decision
    num_rels: 279
    num_bases: 139
    dropout: 0.1
    score_func:
      _target_: model.DistMultScore
    activation:
      _target_: torch.nn.Tanh
  axial_conv:
    # _target_: model.NoAxialTransformer  # 
    _target_: model.AxialTransformer_by_entity
    # emb_size: leave for runtime decision
    _partial_: true
    dropout: 0.0
    num_layers: 6
    heads: 8
  kg_loss_weight: 0.1
  loss_fnt:
    _target_: losses.AFLoss
    gamma_pos: 1.0
    gamma_neg: 1.0
    num_labels: -1

train:
  seed: 66
  epochs: 30
  log_steps: 10                              # 
  device: "cuda:0"
  start_steps: -1
  evaluation_steps: -1                       # 
  gradient_accumulation_steps: 2
  learning_rate: 3e-5
  classifier_lr: 1e-4
  lr_schedule: linear # linear or cosine
  warmup_ratio: 0.2
  adam_epsilon: 1e-6
  max_grad_norm: 1.0
  save_best_path: models/dwie/c1024
  save_last_path: null

load_path: null
load_checkpoint: null
